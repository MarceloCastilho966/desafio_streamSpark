{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21dd63e0",
   "metadata": {},
   "source": [
    "# Desafio Bulk Spark Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909195f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 12:29:00 WARN Utils: Your hostname, marcelo-Aspire-A315-42G resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlp4s0)\n",
      "22/11/01 12:29:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/marcelo/anaconda3/envs/spark/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/marcelo/.ivy2/cache\n",
      "The jars for the packages stored in: /home/marcelo/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9113368b-f954-430c-af82-96eaabadfd38;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 3673ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   1   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9113368b-f954-430c-af82-96eaabadfd38\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/29ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 12:29:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 12:29:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/11/01 12:29:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Criando SparkSession com conector mongo-spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"bulk_stream_spark\")\\\n",
    "    .config(\"spark.logConf\", \"true\")\\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.0.0')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.conf.set('spark.sql.caseSensitive', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e39a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de Schema\n",
    "schema = StructType([StructField('current_observation', \\\n",
    "                                  StructType([StructField('@version', StringType(), True),\\\n",
    "                                              StructField('@xmlns:xsd', StringType(), True),\\\n",
    "                                              StructField('@xmlns:xsi', StringType(), True),\\\n",
    "                                              StructField('@xsi:noNamespaceSchemaLocation', StringType(), True),\\\n",
    "                                              StructField('copyright_url', StringType(), True),\\\n",
    "                                              StructField('credit', StringType(), True),\\\n",
    "                                              StructField('credit_URL', StringType(), True),\\\n",
    "                                              StructField('dewpoint_c', StringType(), True),\\\n",
    "                                              StructField('dewpoint_f', StringType(), True),\\\n",
    "                                              StructField('dewpoint_string', StringType(), True),\\\n",
    "                                              StructField('disclaimer_url', StringType(), True),\\\n",
    "                                              StructField('icon_url_base', StringType(), True),\\\n",
    "                                              StructField('icon_url_name', StringType(), True),\\\n",
    "                                              StructField('image', StructType([\\\n",
    "                                                                               StructField('link', StringType(), True),\\\n",
    "                                                                               StructField('title', StringType(), True),\\\n",
    "                                                                               StructField('url', StringType(), True)]),True),\\\n",
    "                                              StructField('latitude', StringType(), True),\\\n",
    "                                              StructField('location', StringType(), True),\\\n",
    "                                              StructField('longitude', StringType(), True),\\\n",
    "                                              StructField('ob_url', StringType(), True),\\\n",
    "                                              StructField('observation_time', StringType(), True),\\\n",
    "                                              StructField('observation_time_rfc822', StringType(), True),\\\n",
    "                                              StructField('pressure_in', StringType(), True),\\\n",
    "                                              StructField('pressure_mb', StringType(), True),\\\n",
    "                                              StructField('pressure_string', StringType(), True),\\\n",
    "                                              StructField('privacy_policy_url', StringType(), True),\\\n",
    "                                              StructField('relative_humidity', StringType(), True),\\\n",
    "                                              StructField('station_id', StringType(), True),\\\n",
    "                                              StructField('suggested_pickup', StringType(), True),\\\n",
    "                                              StructField('suggested_pickup_period', StringType(), True),\\\n",
    "                                              StructField('temp_c', StringType(), True),\\\n",
    "                                              StructField('temp_f', StringType(), True),\\\n",
    "                                              StructField('temperature_string', StringType(), True),\\\n",
    "                                              StructField('two_day_history_url', StringType(), True),\\\n",
    "                                              StructField('visibility_mi', StringType(), True),\\\n",
    "                                              StructField('weather', StringType(), True),\\\n",
    "                                              StructField('wind_degrees', StringType(), True),\\\n",
    "                                              StructField('wind_dir', StringType(), True),\\\n",
    "                                              StructField('wind_kt', StringType(), True),\\\n",
    "                                              StructField('wind_mph', StringType(), True),\\\n",
    "                                              StructField('wind_string', StringType(), True),\\\n",
    "                                              StructField('windchill_c', StringType(), True),\\\n",
    "                                              StructField('windchill_f', StringType(), True),\\\n",
    "                                              StructField('windchill_string', StringType(), True)]), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b98bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação do Readstream, definindo seu Schema e path\n",
    "df_json = spark.readStream.format(\"json\")\\\n",
    ".schema(schema)\\\n",
    ".load('/home/marcelo/BULK/desafio_streamSpark/files/*json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad601efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando apenas as informações que serão salvas no MongoDB\n",
    "select = df_json.selectExpr(\"CAST (current_observation.location AS STRING) AS Localizacao\",\n",
    "                        \"CAST (current_observation.observation_time_rfc822 AS STRING) AS Data_Hora\",\n",
    "                        \"CAST (current_observation.temp_c AS DOUBLE) AS Temperatura\",\n",
    "                        \"CAST (current_observation.pressure_in AS DOUBLE) AS Pressao\",\n",
    "                        \"CAST (current_observation.weather AS STRING) AS Clima\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcaec9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo diretório do checkpoint e o deletando, caso exista\n",
    "checkpoint = '/home/marcelo/BULK/desafio_streamSpark/checkpoint'\n",
    "if os.path.exists(checkpoint):\n",
    "    shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab022ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 12:29:12 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "22/11/01 12:29:12 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "22/11/01 12:29:12 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Salvando o Stream dos Dados baixados pela API python, no banco de dados MongoDB\n",
    "select.writeStream\\\n",
    "    .format('mongodb')\\\n",
    "    .option('spark.mongodb.connection.uri', 'mongodb://127.0.0.1:27017/')\\\n",
    "    .option('spark.mongodb.database', 'marcelo')\\\n",
    "    .option('spark.mongodb.collection', 'weather')\\\n",
    "    .option(\"checkpointLocation\", checkpoint)\\\n",
    "    .outputMode(\"append\").start().awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
